[data]
#train_feats = /asrDataCenter/dataCenter/asr/td/vx/binaryFormat/110h_traindata_txt/train2.csv
#train_labels = /asrDataCenter/dataCenter/asr/td/vx/binaryFormat/110h_traindata_txt/train2.label.csv
#dev_feats = /asrDataCenter/dataCenter/asr/td/vx/binaryFormat/110h_traindata_txt/cv2.csv
#dev_labels = /asrDataCenter/dataCenter/asr/td/vx/binaryFormat/110h_traindata_txt/cv2.label.csv

train_feats = /home/zhangjl/aiGit/asr/src/mxnet/train2.test.csv
train_labels = /home/zhangjl/aiGit/asr/src/mxnet/train2.test.label.csv
dev_feats = /home/zhangjl/aiGit/asr/src/mxnet/train2.test.csv
dev_labels = /home/zhangjl/aiGit/asr/src/mxnet/train2.test.label.csv

[arch]
num_hidden = 1024
# set it to zero if you want a regular LSTM
num_hidden_proj = 512
num_lstm_layer = 3

[train]
batch_size = 3
num_epoch = 12

# gpu0, gpu1
context = gpu0

# checkpoint prefix
prefix = asr001

learning_rate = 0.01
decay_factor = 2
decay_lower_bound = 1e-6

optimizer = speechSGD
momentum = 0.9

# set to 0 to disable gradient clipping
clip_gradient = 1

# uniform, normal, xavier
initializer = Uniform
init_scale = 0.05
weight_decay = 0.008

# show progress every how many batches
show_every = 1000
